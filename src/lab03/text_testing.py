from ..lib.test import run
from ..lib import text

if __name__ == "__main__":
    run(lambda: text.normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"), result="–ø—Ä–∏–≤–µ—Ç –º–∏—Ä")
    run(lambda: text.normalize("—ë–∂–∏–∫, –Å–ª–∫–∞"), result="–µ–∂–∏–∫, –µ–ª–∫–∞")
    run(lambda: text.normalize("Hello\r\nWorld"), result="hello world")
    run(lambda: text.normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "), result="–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã")

    run(
        lambda: text.normalize("–Å–ª–∫–∏\t–∏–≥–æ–ª–∫–∏", casefold=False, yo2e=False),
        result="–Å–ª–∫–∏ –∏–≥–æ–ª–∫–∏",
    )

    run(lambda: text.tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"), result=["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"])
    run(lambda: text.tokenize("hello,world!!!"), result=["hello", "world"])
    run(lambda: text.tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"), result=["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"])
    run(lambda: text.tokenize("2025 –≥–æ–¥"), result=["2025", "–≥–æ–¥"])

    run(lambda: text.tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"), result=["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"])

    r1 = {"a": 3, "b": 2, "c": 1}
    run(lambda: text.count_freq(["a", "b", "a", "c", "b", "a"]), result=r1)
    run(lambda: text.top_n(r1, n=2), result=[("a", 3), ("b", 2)])

    r2 = {"aa": 2, "bb": 2, "cc": 1}
    run(lambda: text.count_freq(["bb", "aa", "bb", "aa", "cc"]), result=r2)
    run(lambda: text.top_n(r2, n=2), result=[("aa", 2), ("bb", 2)])
